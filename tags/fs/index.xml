<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kingjcy Blog</title>
    <link>http://kingjcy.github.io/tags/fs/index.xml</link>
    <description>Recent content on kingjcy Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2016. All rights reserved.</copyright>
    <atom:link href="http://kingjcy.github.io/tags/fs/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ceph tutorial</title>
      <link>http://kingjcy.github.io/blog/2016/12/01/ceph-tutorial/</link>
      <pubDate>Thu, 01 Dec 2016 14:26:20 +0800</pubDate>
      
      <guid>http://kingjcy.github.io/blog/2016/12/01/ceph-tutorial/</guid>
      <description>&lt;p&gt;ceph是一个linux PB级分布式文件存储系统，它是一个大容量并且简单扩容，高性能，高可靠等特性。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;简析&#34;&gt;简析&lt;/h2&gt;

&lt;p&gt;ceph是一套基于RADOS（可靠自动化分布式对象存储）的分布式对象存储系统。RADOS是由为数众多负责完成数据存储和维护的osd和负责监测的mon组成的完整的对象存储系统。在RADOS上进行抽象和封装形成基础库librados提供上层所需要的API。在librados的基础上继续封装抽象就达到了我们所需要的应用层接口RADOS GW，RBD，ceph FS。在RADOS上进行的完整的对象存储系统。&lt;/p&gt;

&lt;p&gt;集群部署组成部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ceph-deploy   安装部署工具&lt;/li&gt;
&lt;li&gt;ceph mon      监控节点，负责监控，从osd节点获取信息，来判断osd节点的状态&lt;/li&gt;
&lt;li&gt;ceph osd      存储数据&lt;/li&gt;
&lt;li&gt;ceph mds      元数据服务&lt;/li&gt;
&lt;li&gt;ceph client   客户端，用于文件系统&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基本流程：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;寻址流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;file  切割为 object，object是大小是有RADOS来定的，获取到对应到oid，唯一，这个是file到object到第一级映射。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;object 使用算法 hash(oid) &amp;amp; mask -&amp;gt; pgid获取pid完成到pg的映射，这边mask是pg总数-1，最后这种是伪随机的算法，只有在大量的情况下才能产生均衡。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pg 到 osd 的映射，使用的是CRUSH算法。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;三级映射完成数据的查找&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据存储流程&lt;/p&gt;

&lt;p&gt;将file变为一个object，然后找出存储该object的一组三个OSD。这三个OSD具有各自不同的序号，序号最靠前的那个OSD就是这一组中的Primary OSD，而后两个则依次是Secondary OSD和Tertiary OSD。找出三个OSD后，client将直接和Primary OSD通信，发起写入操作（步骤1）。Primary OSD收到请求后，分别向Secondary OSD和Tertiary OSD发起写入操作（步骤2、3）。当Secondary OSD和Tertiary OSD各自完成写入操作后，将分别向Primary OSD发送确认信息（步骤4、5）。当Primary OSD确信其他两个OSD的写入完成后，则自己也完成数据写入，并向client确认object写入操作完成（步骤6）。当各个OSD都将数据写入内存缓冲区后，就先向client发送一次确认，此时client即可以向下执行。解决延迟。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集群维护&lt;/p&gt;

&lt;p&gt;mon共同负责整个Ceph集群中所有OSD状态的发现与记录，并且共同形成cluster map的master版本，然后扩散至全体OSD以及client。OSD使用cluster map进行数据的维护，而client使用cluster map进行数据的寻址。&lt;/p&gt;

&lt;p&gt;monitor并不主动轮询各个OSD的当前状态。正相反，OSD需要向monitor上报状态信息。&lt;/p&gt;

&lt;p&gt;Cluster map: epoch(版本号)，各个osd的网络地址和状态，CRUSH的参数配置。&lt;/p&gt;

&lt;p&gt;Cluster map信息是以增量形式扩散的。如果任意一次通信的双方发现其epoch不一致，则版本更新的一方将把二者所拥有的cluster map的差异发送给另外一方。&lt;/p&gt;

&lt;p&gt;对于一个Ceph集群而言，即便由数千个甚至更多OSD组成，cluster map的数据结构大小也并不惊人。同时，cluster map的状态更新并不会频繁发生。即便如此，Ceph依然对cluster map信息的扩散机制进行了优化，以便减轻相关计算和通信压力。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;技术特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;高可靠&lt;/li&gt;
&lt;li&gt;高自动化&lt;/li&gt;
&lt;li&gt;高扩展&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;设计思路：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;发挥存储设备的计算能力&lt;/li&gt;
&lt;li&gt;去中心化，避免单点故障和瓶颈&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装-centos&#34;&gt;安装（centos）&lt;/h2&gt;

&lt;p&gt;环境准备&lt;/p&gt;

&lt;p&gt;pdapp17     172.32.148.127  ceph-deploy/mon/osd&lt;/p&gt;

&lt;p&gt;pdapp18     172.32.148.128  mds&lt;/p&gt;

&lt;p&gt;pdapp19     172.32.148.129  osd&lt;/p&gt;

&lt;p&gt;pdapp20     172.32.148.130  osd&lt;/p&gt;

&lt;h3 id=&#34;安装-ceph-部署工具&#34;&gt;安装 CEPH 部署工具&lt;/h3&gt;

&lt;p&gt;首先安装ceph-deploy这个ceph部署管理工具，便于ceph的安装。&lt;/p&gt;

&lt;p&gt;创建源文件&lt;/p&gt;

&lt;p&gt;sudo vim /etc/yum.repos.d/ceph.repo 追加以下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ceph]
name=Ceph packages for $basearch
baseurl=https://download.ceph.com/rpm-jewel/el7/x86_64
enabled=1
priority=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

[ceph-noarch]
name=Ceph noarch packages
baseurl=https://download.ceph.com/rpm-jewel/el7/noarch
enabled=1
priority=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

[ceph-source]
name=Ceph source packages
baseurl=https://download.ceph.com/rpm-jewel/el7/SRPMS
enabled=0
priority=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是ceph的官方源，其中ceoh-deploy应该是在源[ceph-noarch]，这边一并加了后面要用这个源，（但是这些源都太慢了，我把对应的rpm包下载下来(可以第一次用这个源安装，但是修改源的配置文件保留下载的安装包)做了一个内部源，这样就可以快速的完成安装了,建议可以这样做）&lt;/p&gt;

&lt;p&gt;最后更新安装就好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum update &amp;amp;&amp;amp; sudo yum install ceph-deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边管理节点就安装好了，比较简单，下面开始安装ceph。&lt;/p&gt;

&lt;h3 id=&#34;ceph-节点安装和集群部署&#34;&gt;CEPH 节点安装和集群部署&lt;/h3&gt;

&lt;h4 id=&#34;1-集群机器环境准备&#34;&gt;1. 集群机器环境准备&lt;/h4&gt;

&lt;p&gt;安装ceph-deploy的管理节点必须能够通过 SSH 无密码地访问各 Ceph 节点。如果 ceph-deploy 以某个普通用户安装，那么这个用户必须有无密码使用 sudo 的权限。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装ntp&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;安装ntp始终服务可以避免时钟漂移产生故障。所有的集群机器都要安装，在centos下只要用用yum安装就好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install ntp ntpdate ntp-doc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要对ntp服务进行设置，修改配置文件/etc/ntp.conf，一台主机时间作为标准时间来同步，集群其他的机器来和这台机器时间进行同步&lt;/p&gt;

&lt;p&gt;标准时间机器也就是ntpserver配置新增&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server  127.127.1.0     # local clock
fudge   127.127.1.0 stratum 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示以本地时间为准，然后启动或者重启ntp服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl start ntpd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群的其他机器需要修改配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server ip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群的其他机器第一次需要手动同步，然后启动ntp服务，手动同步直接用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ntpdate ip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同步成功输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;16 Dec 16:29:21 ntpdate[29461]: adjust time server 172.32.148.127 offset 0.000601 sec
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装ssh&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;确保系统中有ssh服务，否则需要安装，安装很简单，直接使用yum&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install openssh-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有ssh服务需要确保服务都在运行&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;创建账户并且支持各个节点之间对无密码ssh登陆&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ceph-deploy 工具必须以普通用户登录 Ceph 节点，且此用户拥有无密码使用 sudo 的权限，因为它需要在安装软件及配置文件的过程中，不必输入密码。&lt;/p&gt;

&lt;p&gt;新版ceph-deploy支持使用&amp;ndash;username来创建无密码登陆适用于ssh用户。用户名最好用一些自己取对名字，增加安全性。然后针对自己对用户赋予sudo权限。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;创建用户&lt;/p&gt;

&lt;p&gt;sudo useradd -d /home/{username} -m {username}
passwd {username}&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;增加root权限&lt;/p&gt;

&lt;p&gt;echo &amp;ldquo;{username} ALL = (ALL)ALL&amp;rdquo; | sudo tee /etc/sudoers&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;无密码ssh登陆&lt;/p&gt;

&lt;p&gt;ssh-keygen  生成密钥
##scp ~/.ssh/id_rsa.pub xxx@host:/home/xxx/id_rsa.pub   复制到其他主机上来完成无密码访问有一个更好用到命令&lt;/p&gt;

&lt;p&gt;ssh-copy-id {username}@hostname   将密钥~/.ssh/id_rsa.pub复制到其他主机上文件为authorized_keys中，就可以无密码登陆了。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;解析主机名&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编辑/etc/hosts加上对于到ip 主机名 就可以直接用主机名进行访问了，也可以给主机起别名，编辑~/.ssh/config&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Host node1
    Hostname node1
    User {username}
Host node2
    Hostname node2          
    User {username}
Host node3
    Hostname node3
    User {username}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装过程中需要关闭防火墙&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl stop firewalld
setenforce 0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装优先级/首选项&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install yum-plugin-priorities
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-安装ceph部署集群&#34;&gt;2.安装ceph部署集群&lt;/h4&gt;

&lt;p&gt;集群的基础环境已经部署好了，下面就是开始安装ceph，部署集群。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;登陆ceph-deploy主机先建一个目录来存储集群需要的配置文件和密钥&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir my-cluster
cd my-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个过程中需要ssh到集群其他机器还需要使用这个机器到sudo权限，所以需要修改配置文件/etc/sudoers,屏蔽掉#Defaults    requiretty才行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;使用ceph-deploy来创建集群&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy new [-h] [--no-ssh-copykey] [--fsid FSID][--cluster-network CLUSTER_NETWORK][--public-network PUBLIC_NETWORK] MON [MON ...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用这个命令开始部署一个集群，后面的主机名也是接下来要部署的mon节点的主机名，这边指定主机，主要是生成一个配置文件和一个密钥。后面需要初始化。在这还需要修改一下配置文件&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;修改osd节点的个数默认为3，这边改为2&lt;/p&gt;

&lt;p&gt;osd pool default size = 2&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置网卡，这个不是一定要配置的&lt;/p&gt;

&lt;p&gt;public network = {ip-address}/{netmask}&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装ceph&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;使用ceph-deploy来安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy install [-h] [--stable [CODENAME] | --release [CODENAME] |--testing | --dev [BRANCH_OR_TAG] | --dev-commit[COMMIT]] [--mon] [--mds] [--rgw] [--osd] [--tests][--cli] [--all][--adjust-repos | --no-adjust-repos | --repo][--local-mirror [LOCAL_MIRROR]][--repo-url [REPO_URL]] [--gpg-url [GPG_URL][--nogpgcheck] HOST [HOST ...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是在各个host安装ceph,先检验源，如果没有则安装一个ceph.repo，我这边用了内部源，则检测到不会被覆盖，然后主要是安装ceph：sudo yum -y install ceph ceph-radosgw。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;初始化mon节点&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy mon create-initial
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时在ceph-deploy主机集群目录下会生成对应的密钥。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;添加OSD节点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先在osd节点的主机上新建目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;login pdapp19  
mkdir /ceph/osd0
login pdapp20
mkdir /ceph/osd1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新建逻辑卷挂载到这个目录上,在对应的osd主机上执行分区挂载。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;umount /home &amp;amp;&amp;amp; lvremove /dev/centos/home &amp;amp;&amp;amp; mkdir /ceph &amp;amp;&amp;amp; lvcreate -L 107374182400k -n ceph centos &amp;amp;&amp;amp; mkfs.xfs -i size=512 /dev/centos/ceph &amp;amp;&amp;amp; mount /dev/centos/ceph /ceph;df -h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后管理节点执行 ceph-deploy 来准备 OSD,这边特别主要的是各个机器的主机的防火墙一定要关闭，还有对应的文件是否有其他用户执行权限没有则使用chmod来修改。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-delpoy osd prepare pdapp19:/ceph/osd0 pdapp20:/ceph/osd1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;激活osd节点，最终会在osd节点上创建systemd 服务单元&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy osd activate pdapp19:/ceph/osd0 pdapp20:/ceph/osd1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和 ceph.client.admin.keyring 了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy admin pdapp17 pdapp18 pdapp19 pdapp20

sudo chmod +r /etc/ceph/ceph.client.admin.keyring
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群就ok了，查看一下集群状况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph health/ceph -s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显示的应该是达到 active + clean 状态。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;3-集群卸载&#34;&gt;3.集群卸载&lt;/h3&gt;

&lt;p&gt;卸载ceph&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy purge {ceph-node}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个过程中主要是卸载ceph相关软件：sudo yum -y -q remove ceph ceph-release ceph-common ceph-radosgw  还将ceph源备份，因为在安装的时候是安装ceph源：/etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave&lt;/p&gt;

&lt;p&gt;清除相关数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy purgedata {ceph-node}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要是删除之前安装时在相关目录下产生的文件，可以通过手动直接删除这个文件来实现：sudo rm -rf &amp;ndash;one-file-system &amp;ndash; /var/lib/ceph &amp;amp;&amp;amp; sudo rm -rf &amp;ndash;one-file-system &amp;ndash; /etc/ceph/&lt;/p&gt;

&lt;p&gt;最后清除key&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy forgetkeys
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要对osd单独分配的逻辑卷进行擦除，先直接删除对应的osdN中的所有文件，然后擦除逻辑卷&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy disk zap hostname:lvname
ceph-deploy disk zap pdapp19:/dev/mapper/centos-ceph
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;操作集群&#34;&gt;操作集群&lt;/h2&gt;

&lt;h3 id=&#34;启动集群&#34;&gt;启动集群&lt;/h3&gt;

&lt;p&gt;启动所有的deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl start ceph.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动一个类型的所有deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl start ceph-osd.target
sudo systemctl start ceph-mon.target
sudo systemctl start ceph-mds.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动单独的一个deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl start ceph-osd@{id}
sudo systemctl start ceph-mon@{hostname}
sudo systemctl start ceph-mds@{hostname}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止集群&lt;/p&gt;

&lt;p&gt;停止所有的deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl stop ceph\*.service ceph\*.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止一个类型的所有deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl stop ceph-mon\*.service ceph-mon.target
sudo systemctl stop ceph-osd\*.service ceph-osd.target
sudo systemctl stop ceph-mds\*.service ceph-mds.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止单独一个deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl stop ceph-osd@{id}
sudo systemctl stop ceph-mon@{hostname}
sudo systemctl stop ceph-mds@{hostname}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看集群状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl status ceph\*.service ceph\*.target
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;扩展集群&#34;&gt;扩展集群&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;新增osd节点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;新增节点和一开始部署osd节点是一样的。这边以在pdapp17上新增osd节点为例。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;建目录&lt;/p&gt;

&lt;p&gt;login pdapp17
mkdir /ceph/osd2&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重新划分逻辑券来挂载&lt;/p&gt;

&lt;p&gt;umount /home &amp;amp;&amp;amp; lvremove /dev/centos/home &amp;amp;&amp;amp; lvcreate -L 52428800k -n ceph centos &amp;amp;&amp;amp; mkfs.xfs -i size=512 /dev/centos/ceph &amp;amp;&amp;amp; mount /dev/centos/ceph /ceph;df -h&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3.准备osd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy osd prepare pdapp17:/ceph/osd2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.激活osd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy osd activate pdapp17:/ceph/osd2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用ceph -w来观察集群的变化。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;新增mds节点&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy mds create {ceph-node}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;新增rgw例程&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy rgw create {getway-node}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;rgw例程默认端口是7480，可以修改ceph.conf来修改端口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[client]
rgw frontends = civetweb port=80
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;增加mon节点&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy mon add {ceph-node}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ceph文件系统&#34;&gt;Ceph文件系统&lt;/h2&gt;

&lt;p&gt;首先需要创建mds&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>fastdfs tutorial</title>
      <link>http://kingjcy.github.io/blog/2016/01/16/fastdfs-tutorial/</link>
      <pubDate>Sat, 16 Jan 2016 10:46:35 +0800</pubDate>
      
      <guid>http://kingjcy.github.io/blog/2016/01/16/fastdfs-tutorial/</guid>
      <description>

&lt;p&gt;fastdfs是一个分布式存储系统&lt;/p&gt;

&lt;h2 id=&#34;安装-v5-08&#34;&gt;安装（v5.08）&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;环境准备&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;172.32.148.127 client&lt;/p&gt;

&lt;p&gt;172.32.148.128 storage&lt;/p&gt;

&lt;p&gt;172.32.148.129 tracker storage&lt;/p&gt;

&lt;p&gt;172.32.148.130 storage&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;依赖libfastcommon安装&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;v5版本已经不需要单独安装libevent，但是需要安装项目中的libfastcommon，所有机器都要安装，下载地址&lt;/p&gt;

&lt;p&gt;git clone &lt;a href=&#34;https://github.com/happyfish100/libfastcommon.git&#34;&gt;https://github.com/happyfish100/libfastcommon.git&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;然后用自带的脚本make.sh编译安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./make.sh
./make.sh install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;fastdfs安装&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;依赖安装好就开始安装fastdfs，下载压缩包fastdfs-5.08.tar.gz，所有机器进行安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./make.sh
./make.sh install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;配置文件与进程&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;fastdfs安装好后在/etc/fdfs/下面会有对应的配置文件，这个项目的配置文件注释还是比较全面的，对它进行修改&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;tracker.conf&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;监控进程使用的配置文件，主要是配置端口，日志数据存储路径，端口一般使用配置文件默认的22122，在172.32.148.129上操作&lt;/p&gt;

&lt;h1 id=&#34;the-tracker-server-port&#34;&gt;the tracker server port&lt;/h1&gt;

&lt;p&gt;port=22122&lt;/p&gt;

&lt;h1 id=&#34;the-base-path-to-store-data-and-log-files&#34;&gt;the base path to store data and log files&lt;/h1&gt;

&lt;p&gt;base_path=/home/jcy/fastdfs/fastdfs_tracker&lt;/p&gt;

&lt;p&gt;然后启动监控进程，到对应的路径下看日志&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fdfs_trackerd /etc/fdfs/tracker.conf start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以ps看一下进程启动状况，也可以到日志看启动状况，也可以通过netstat来看&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;netstat -tupln|grep tracker
#可以看到如下：
tcp  0   0   0.0.0.0:22122   0.0.0.0:*   LISTEN   18559/fdfs_trackerd
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;storage.conf&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;存储进程使用的配置文件，主要是指定tracker服务的地址和端口，因为存储进程要定时给监控进程发送数据信息，同group内的storage进程会相互connect，来同步文件。这些进程在172.32.148.128，172.32.148.129，172.32.148.130上进行操作。同时它也有自己的group_name和开发的端口，还有对应的日志数据路径。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# the name of the group this storage server belongs to
#
# comment or remove this item for fetching from tracker server,
# in this case, use_storage_id must set to true in tracker.conf,
# and storage_ids.conf must be configed correctly.
group_name=group1

# the storage server port
port=23000

# the base path to store data and log files
base_path=/home/jcy/fastdfs/fastdfs_storage

# store_path#, based 0, if store_path0 not exists, it&#39;s value is base_path
# the paths must be exist
store_path0=/home/jcy/fastdfs/fastdfs_storage

# tracker_server can ocur more than once, and tracker_server format is
#  &amp;quot;host:port&amp;quot;, host can be hostname or ip address
tracker_server=172.32.148.129:22122
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动存储进程，查看对应的情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fdfs_storaged /etc/fdfs/storage.conf start
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;client.conf&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;客户端进行文件上传的测试,首先是对配置文件的tracker服务器地址进行配置，然后就是一些基本配置看注释就可以&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;测试文件上传&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在客户端机器172.32.148.127上进行操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[jcy@pdapp17 fastdfs]$ fdfs_upload_file /etc/fdfs/client.conf a.txt
group1/M00/00/00/rCCUgVh8lF-ATRZpAAAADw_r4o4559.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上传成功，到对应的storage进程指定的path下的data目录就可以找到对应的文件，文件会同时同步的同一个group下的所有storage上，完成备份。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>