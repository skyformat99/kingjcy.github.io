<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kingjcy Blog</title>
    <link>http://kingjcy.github.io/tags/ceph/index.xml</link>
    <description>Recent content on kingjcy Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2016. All rights reserved.</copyright>
    <atom:link href="http://kingjcy.github.io/tags/ceph/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ceph管理平台calamari的安装</title>
      <link>http://kingjcy.github.io/blog/2016/12/21/ceph%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0calamari%E7%9A%84%E5%AE%89%E8%A3%85/</link>
      <pubDate>Wed, 21 Dec 2016 16:17:28 +0800</pubDate>
      
      <guid>http://kingjcy.github.io/blog/2016/12/21/ceph%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0calamari%E7%9A%84%E5%AE%89%E8%A3%85/</guid>
      <description>&lt;p&gt;环境准备&lt;/p&gt;

&lt;p&gt;pdapp17     172.32.148.127  ceph-deploy/mon/osd&lt;/p&gt;

&lt;p&gt;pdapp18     172.32.148.128  mds&lt;/p&gt;

&lt;p&gt;pdapp19     172.32.148.129  osd&lt;/p&gt;

&lt;p&gt;pdapp20     172.32.148.130  osd&lt;/p&gt;

&lt;p&gt;获取安装包&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pdapp17 calamari]# l
总用量 21436
-rw-r--r--. 1 root root 20283376 12月 21 15:15 calamari-server-1.3.1.1-105_g79c8df2.el7.centos.x86_64.rpm
-rw-r--r--. 1 root root     3661 12月 21 15:15 ceph-deploy-ceph.log
-rw-r--r--. 1 root root   558644 12月 21 15:15 diamond-3.4.67-0.noarch.rpm
-rw-r--r--. 1 root root  1099116 12月 21 15:15 romana-1.2.2-36_gc62bb5b.el7.centos.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在pdapp17上安装calamari-server-1.3.1.1-105_g79c8df2.el7.centos.x86_64.rpm&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install -y calamari-server-1.3.1.1-105_g79c8df2.el7.centos.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初始化calamari服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;calamari-ctl initialize
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在所有的机器上安装diamond-3.4.67-0.noarch.rpm&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install -y diamond-3.4.67-0.noarch.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改所有机器的diamond的配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /etc/diamond/
mv diamond.conf.example diamond.conf
vi diamond.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要将host主机修改为安装calamari服务安装的主机名&lt;/p&gt;

&lt;p&gt;重启diamond服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl start diamond
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;重启之前需要加载systemd的配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在所有的主机主机上安装salt-minion&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install -y salt-minion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后修改配置文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /etc/salt/minion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将master全部修改为calamari安装的主机名&lt;/p&gt;

&lt;p&gt;然后重启diamond服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl restart diamond
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启懂salt-minion服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl start salt-minion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在安装过程中全部使用root，并且关闭所有机器的防火墙。&lt;/p&gt;

&lt;p&gt;修改python脚本&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /opt/calamari/salt/salt/_modules/ceph.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改下面三行，在571行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mon_epoch = status.get(&#39;monmap&#39;, {}).get(&#39;epoch&#39;)
osd_epoch = status.get(&#39;osdmap&#39;, {}).get(&#39;osdmap&#39;, {}).get(&#39;epoch&#39;)
mds_epoch = status.get(&#39;fsmap&#39;, status.get(&#39;mdsmap&#39;, {})).get(&#39;epoch&#39;)&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后使用salt命令来查看机器的情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@pdapp17 salt]# salt-key -L
Accepted Keys:
Denied Keys:
Unaccepted Keys:
pdapp17
pdapp18
pdapp19
pdapp20
Rejected Keys:
[root@pdapp17 salt]# salt-key -A
The following keys are going to be accepted:
Unaccepted Keys:
pdapp17
pdapp18
pdapp19
pdapp20
Proceed? [n/Y] y
Key for minion pdapp17 accepted.
Key for minion pdapp18 accepted.
Key for minion pdapp19 accepted.
Key for minion pdapp20 accepted.
[root@pdapp17 salt]# salt-key -L
Accepted Keys:
pdapp17
pdapp18
pdapp19
pdapp20
Denied Keys:
Unaccepted Keys:
Rejected Keys:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;分别使用下面的命令来检验各台机器的情况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;salt &amp;quot;*&amp;quot; test.ping
salt &amp;quot;*&amp;quot; ceph.get_heartbeats
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在calamari主机上安装romana-1.2.2-36_gc62bb5b.el7.centos.x86_64.rpm&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum install -y romana-1.2.2-36_gc62bb5b.el7.centos.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改日志的权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod 777 /var/log/calamari/*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后在这台机器上重启&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl restart supervisord.service
systemctl restart httpd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就可以直接通过这台主机的ip来访问了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ceph tutorial</title>
      <link>http://kingjcy.github.io/blog/2016/12/01/ceph-tutorial/</link>
      <pubDate>Thu, 01 Dec 2016 14:26:20 +0800</pubDate>
      
      <guid>http://kingjcy.github.io/blog/2016/12/01/ceph-tutorial/</guid>
      <description>&lt;p&gt;ceph是一个linux PB级分布式文件存储系统，它是一个大容量并且简单扩容，高性能，高可靠等特性。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;简析&#34;&gt;简析&lt;/h2&gt;

&lt;p&gt;ceph是一套基于RADOS（可靠自动化分布式对象存储）的分布式对象存储系统。RADOS是由为数众多负责完成数据存储和维护的osd和负责监测的mon组成的完整的对象存储系统。在RADOS上进行抽象和封装形成基础库librados提供上层所需要的API。在librados的基础上继续封装抽象就达到了我们所需要的应用层接口RADOS GW，RBD，ceph FS。在RADOS上进行的完整的对象存储系统。&lt;/p&gt;

&lt;p&gt;集群部署组成部分：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ceph-deploy   安装部署工具&lt;/li&gt;
&lt;li&gt;ceph mon      监控节点，负责监控，从osd节点获取信息，来判断osd节点的状态&lt;/li&gt;
&lt;li&gt;ceph osd      存储数据&lt;/li&gt;
&lt;li&gt;ceph mds      元数据服务&lt;/li&gt;
&lt;li&gt;ceph client   客户端，用于文件系统&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基本流程：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;寻址流程&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;file  切割为 object，object是大小是有RADOS来定的，获取到对应到oid，唯一，这个是file到object到第一级映射。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;object 使用算法 hash(oid) &amp;amp; mask -&amp;gt; pgid获取pid完成到pg的映射，这边mask是pg总数-1，最后这种是伪随机的算法，只有在大量的情况下才能产生均衡。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;pg 到 osd 的映射，使用的是CRUSH算法。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;三级映射完成数据的查找&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;数据存储流程&lt;/p&gt;

&lt;p&gt;将file变为一个object，然后找出存储该object的一组三个OSD。这三个OSD具有各自不同的序号，序号最靠前的那个OSD就是这一组中的Primary OSD，而后两个则依次是Secondary OSD和Tertiary OSD。找出三个OSD后，client将直接和Primary OSD通信，发起写入操作（步骤1）。Primary OSD收到请求后，分别向Secondary OSD和Tertiary OSD发起写入操作（步骤2、3）。当Secondary OSD和Tertiary OSD各自完成写入操作后，将分别向Primary OSD发送确认信息（步骤4、5）。当Primary OSD确信其他两个OSD的写入完成后，则自己也完成数据写入，并向client确认object写入操作完成（步骤6）。当各个OSD都将数据写入内存缓冲区后，就先向client发送一次确认，此时client即可以向下执行。解决延迟。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;集群维护&lt;/p&gt;

&lt;p&gt;mon共同负责整个Ceph集群中所有OSD状态的发现与记录，并且共同形成cluster map的master版本，然后扩散至全体OSD以及client。OSD使用cluster map进行数据的维护，而client使用cluster map进行数据的寻址。&lt;/p&gt;

&lt;p&gt;monitor并不主动轮询各个OSD的当前状态。正相反，OSD需要向monitor上报状态信息。&lt;/p&gt;

&lt;p&gt;Cluster map: epoch(版本号)，各个osd的网络地址和状态，CRUSH的参数配置。&lt;/p&gt;

&lt;p&gt;Cluster map信息是以增量形式扩散的。如果任意一次通信的双方发现其epoch不一致，则版本更新的一方将把二者所拥有的cluster map的差异发送给另外一方。&lt;/p&gt;

&lt;p&gt;对于一个Ceph集群而言，即便由数千个甚至更多OSD组成，cluster map的数据结构大小也并不惊人。同时，cluster map的状态更新并不会频繁发生。即便如此，Ceph依然对cluster map信息的扩散机制进行了优化，以便减轻相关计算和通信压力。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;技术特性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;高可靠&lt;/li&gt;
&lt;li&gt;高自动化&lt;/li&gt;
&lt;li&gt;高扩展&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;设计思路：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;发挥存储设备的计算能力&lt;/li&gt;
&lt;li&gt;去中心化，避免单点故障和瓶颈&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;安装-centos&#34;&gt;安装（centos）&lt;/h2&gt;

&lt;p&gt;环境准备&lt;/p&gt;

&lt;p&gt;pdapp17     172.32.148.127  ceph-deploy/mon/osd&lt;/p&gt;

&lt;p&gt;pdapp18     172.32.148.128  mds&lt;/p&gt;

&lt;p&gt;pdapp19     172.32.148.129  osd&lt;/p&gt;

&lt;p&gt;pdapp20     172.32.148.130  osd&lt;/p&gt;

&lt;h3 id=&#34;安装-ceph-部署工具&#34;&gt;安装 CEPH 部署工具&lt;/h3&gt;

&lt;p&gt;首先安装ceph-deploy这个ceph部署管理工具，便于ceph的安装。&lt;/p&gt;

&lt;p&gt;创建源文件&lt;/p&gt;

&lt;p&gt;sudo vim /etc/yum.repos.d/ceph.repo 追加以下内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[ceph]
name=Ceph packages for $basearch
baseurl=https://download.ceph.com/rpm-jewel/el7/x86_64
enabled=1
priority=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

[ceph-noarch]
name=Ceph noarch packages
baseurl=https://download.ceph.com/rpm-jewel/el7/noarch
enabled=1
priority=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

[ceph-source]
name=Ceph source packages
baseurl=https://download.ceph.com/rpm-jewel/el7/SRPMS
enabled=0
priority=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个是ceph的官方源，其中ceoh-deploy应该是在源[ceph-noarch]，这边一并加了后面要用这个源，（但是这些源都太慢了，我把对应的rpm包下载下来(可以第一次用这个源安装，但是修改源的配置文件保留下载的安装包)做了一个内部源，这样就可以快速的完成安装了,建议可以这样做）&lt;/p&gt;

&lt;p&gt;最后更新安装就好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum update &amp;amp;&amp;amp; sudo yum install ceph-deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;到这边管理节点就安装好了，比较简单，下面开始安装ceph。&lt;/p&gt;

&lt;h3 id=&#34;ceph-节点安装和集群部署&#34;&gt;CEPH 节点安装和集群部署&lt;/h3&gt;

&lt;h4 id=&#34;1-集群机器环境准备&#34;&gt;1. 集群机器环境准备&lt;/h4&gt;

&lt;p&gt;安装ceph-deploy的管理节点必须能够通过 SSH 无密码地访问各 Ceph 节点。如果 ceph-deploy 以某个普通用户安装，那么这个用户必须有无密码使用 sudo 的权限。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装ntp&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;安装ntp始终服务可以避免时钟漂移产生故障。所有的集群机器都要安装，在centos下只要用用yum安装就好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install ntp ntpdate ntp-doc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要对ntp服务进行设置，修改配置文件/etc/ntp.conf，一台主机时间作为标准时间来同步，集群其他的机器来和这台机器时间进行同步&lt;/p&gt;

&lt;p&gt;标准时间机器也就是ntpserver配置新增&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server  127.127.1.0     # local clock
fudge   127.127.1.0 stratum 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;表示以本地时间为准，然后启动或者重启ntp服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl start ntpd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群的其他机器需要修改配置&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server ip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群的其他机器第一次需要手动同步，然后启动ntp服务，手动同步直接用&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ntpdate ip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同步成功输出&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;16 Dec 16:29:21 ntpdate[29461]: adjust time server 172.32.148.127 offset 0.000601 sec
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装ssh&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;确保系统中有ssh服务，否则需要安装，安装很简单，直接使用yum&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install openssh-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有ssh服务需要确保服务都在运行&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;创建账户并且支持各个节点之间对无密码ssh登陆&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ceph-deploy 工具必须以普通用户登录 Ceph 节点，且此用户拥有无密码使用 sudo 的权限，因为它需要在安装软件及配置文件的过程中，不必输入密码。&lt;/p&gt;

&lt;p&gt;新版ceph-deploy支持使用&amp;ndash;username来创建无密码登陆适用于ssh用户。用户名最好用一些自己取对名字，增加安全性。然后针对自己对用户赋予sudo权限。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;创建用户&lt;/p&gt;

&lt;p&gt;sudo useradd -d /home/{username} -m {username}
passwd {username}&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;增加root权限&lt;/p&gt;

&lt;p&gt;echo &amp;ldquo;{username} ALL = (ALL)ALL&amp;rdquo; | sudo tee /etc/sudoers&amp;rdquo;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;无密码ssh登陆&lt;/p&gt;

&lt;p&gt;ssh-keygen  生成密钥
##scp ~/.ssh/id_rsa.pub xxx@host:/home/xxx/id_rsa.pub   复制到其他主机上来完成无密码访问有一个更好用到命令&lt;/p&gt;

&lt;p&gt;ssh-copy-id {username}@hostname   将密钥~/.ssh/id_rsa.pub复制到其他主机上文件为authorized_keys中，就可以无密码登陆了。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;解析主机名&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;编辑/etc/hosts加上对于到ip 主机名 就可以直接用主机名进行访问了，也可以给主机起别名，编辑~/.ssh/config&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Host node1
    Hostname node1
    User {username}
Host node2
    Hostname node2          
    User {username}
Host node3
    Hostname node3
    User {username}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装过程中需要关闭防火墙&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl stop firewalld
setenforce 0
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装优先级/首选项&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo yum install yum-plugin-priorities
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-安装ceph部署集群&#34;&gt;2.安装ceph部署集群&lt;/h4&gt;

&lt;p&gt;集群的基础环境已经部署好了，下面就是开始安装ceph，部署集群。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;登陆ceph-deploy主机先建一个目录来存储集群需要的配置文件和密钥&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir my-cluster
cd my-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这个过程中需要ssh到集群其他机器还需要使用这个机器到sudo权限，所以需要修改配置文件/etc/sudoers,屏蔽掉#Defaults    requiretty才行。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;使用ceph-deploy来创建集群&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy new [-h] [--no-ssh-copykey] [--fsid FSID][--cluster-network CLUSTER_NETWORK][--public-network PUBLIC_NETWORK] MON [MON ...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用这个命令开始部署一个集群，后面的主机名也是接下来要部署的mon节点的主机名，这边指定主机，主要是生成一个配置文件和一个密钥。后面需要初始化。在这还需要修改一下配置文件&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;修改osd节点的个数默认为3，这边改为2&lt;/p&gt;

&lt;p&gt;osd pool default size = 2&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;配置网卡，这个不是一定要配置的&lt;/p&gt;

&lt;p&gt;public network = {ip-address}/{netmask}&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;安装ceph&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;使用ceph-deploy来安装&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy install [-h] [--stable [CODENAME] | --release [CODENAME] |--testing | --dev [BRANCH_OR_TAG] | --dev-commit[COMMIT]] [--mon] [--mds] [--rgw] [--osd] [--tests][--cli] [--all][--adjust-repos | --no-adjust-repos | --repo][--local-mirror [LOCAL_MIRROR]][--repo-url [REPO_URL]] [--gpg-url [GPG_URL][--nogpgcheck] HOST [HOST ...]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;就是在各个host安装ceph,先检验源，如果没有则安装一个ceph.repo，我这边用了内部源，则检测到不会被覆盖，然后主要是安装ceph：sudo yum -y install ceph ceph-radosgw。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;初始化mon节点&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy mon create-initial
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同时在ceph-deploy主机集群目录下会生成对应的密钥。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;添加OSD节点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先在osd节点的主机上新建目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;login pdapp19  
mkdir /ceph/osd0
login pdapp20
mkdir /ceph/osd1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;新建逻辑卷挂载到这个目录上,在对应的osd主机上执行分区挂载。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;umount /home &amp;amp;&amp;amp; lvremove /dev/centos/home &amp;amp;&amp;amp; mkdir /ceph &amp;amp;&amp;amp; lvcreate -L 107374182400k -n ceph centos &amp;amp;&amp;amp; mkfs.xfs -i size=512 /dev/centos/ceph &amp;amp;&amp;amp; mount /dev/centos/ceph /ceph;df -h
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后管理节点执行 ceph-deploy 来准备 OSD,这边特别主要的是各个机器的主机的防火墙一定要关闭，还有对应的文件是否有其他用户执行权限没有则使用chmod来修改。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-delpoy osd prepare pdapp19:/ceph/osd0 pdapp20:/ceph/osd1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;激活osd节点，最终会在osd节点上创建systemd 服务单元&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy osd activate pdapp19:/ceph/osd0 pdapp20:/ceph/osd1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和 ceph.client.admin.keyring 了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy admin pdapp17 pdapp18 pdapp19 pdapp20

sudo chmod +r /etc/ceph/ceph.client.admin.keyring
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;集群就ok了，查看一下集群状况&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph health/ceph -s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显示的应该是达到 active + clean 状态。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;3-集群卸载&#34;&gt;3.集群卸载&lt;/h3&gt;

&lt;p&gt;卸载ceph&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy purge {ceph-node}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个过程中主要是卸载ceph相关软件：sudo yum -y -q remove ceph ceph-release ceph-common ceph-radosgw  还将ceph源备份，因为在安装的时候是安装ceph源：/etc/yum.repos.d/ceph.repo saved as /etc/yum.repos.d/ceph.repo.rpmsave&lt;/p&gt;

&lt;p&gt;清除相关数据&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy purgedata {ceph-node}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;主要是删除之前安装时在相关目录下产生的文件，可以通过手动直接删除这个文件来实现：sudo rm -rf &amp;ndash;one-file-system &amp;ndash; /var/lib/ceph &amp;amp;&amp;amp; sudo rm -rf &amp;ndash;one-file-system &amp;ndash; /etc/ceph/&lt;/p&gt;

&lt;p&gt;最后清除key&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy forgetkeys
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;需要对osd单独分配的逻辑卷进行擦除，先直接删除对应的osdN中的所有文件，然后擦除逻辑卷&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy disk zap hostname:lvname
ceph-deploy disk zap pdapp19:/dev/mapper/centos-ceph
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;操作集群&#34;&gt;操作集群&lt;/h2&gt;

&lt;h3 id=&#34;启动集群&#34;&gt;启动集群&lt;/h3&gt;

&lt;p&gt;启动所有的deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl start ceph.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动一个类型的所有deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl start ceph-osd.target
sudo systemctl start ceph-mon.target
sudo systemctl start ceph-mds.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动单独的一个deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl start ceph-osd@{id}
sudo systemctl start ceph-mon@{hostname}
sudo systemctl start ceph-mds@{hostname}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止集群&lt;/p&gt;

&lt;p&gt;停止所有的deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl stop ceph\*.service ceph\*.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止一个类型的所有deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl stop ceph-mon\*.service ceph-mon.target
sudo systemctl stop ceph-osd\*.service ceph-osd.target
sudo systemctl stop ceph-mds\*.service ceph-mds.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止单独一个deamon&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl stop ceph-osd@{id}
sudo systemctl stop ceph-mon@{hostname}
sudo systemctl stop ceph-mds@{hostname}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看集群状态&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl status ceph\*.service ceph\*.target
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;扩展集群&#34;&gt;扩展集群&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;新增osd节点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;新增节点和一开始部署osd节点是一样的。这边以在pdapp17上新增osd节点为例。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;建目录&lt;/p&gt;

&lt;p&gt;login pdapp17
mkdir /ceph/osd2&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重新划分逻辑券来挂载&lt;/p&gt;

&lt;p&gt;umount /home &amp;amp;&amp;amp; lvremove /dev/centos/home &amp;amp;&amp;amp; lvcreate -L 52428800k -n ceph centos &amp;amp;&amp;amp; mkfs.xfs -i size=512 /dev/centos/ceph &amp;amp;&amp;amp; mount /dev/centos/ceph /ceph;df -h&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3.准备osd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy osd prepare pdapp17:/ceph/osd2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4.激活osd&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy osd activate pdapp17:/ceph/osd2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以使用ceph -w来观察集群的变化。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;新增mds节点&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy mds create {ceph-node}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;新增rgw例程&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy rgw create {getway-node}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;rgw例程默认端口是7480，可以修改ceph.conf来修改端口&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[client]
rgw frontends = civetweb port=80
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;增加mon节点&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ceph-deploy mon add {ceph-node}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ceph文件系统&#34;&gt;Ceph文件系统&lt;/h2&gt;

&lt;p&gt;首先需要创建mds&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>